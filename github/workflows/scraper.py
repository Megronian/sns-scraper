#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
import time
import os

class SNSScraper:
    def __init__(self):
        # ãƒ–ãƒ©ã‚¦ã‚¶ã®ãµã‚Šã‚’ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒƒãƒ€ãƒ¼
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.data = []
        
        # dataãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        os.makedirs('data', exist_ok=True)
    
    def safe_request(self, url, max_retries=3):
        """å®‰å…¨ã«HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        for attempt in range(max_retries):
            try:
                print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
                time.sleep(2)  # 2ç§’å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ãï¼‰
                response = requests.get(url, headers=self.headers, timeout=15)
                if response.status_code == 200:
                    return response
                else:
                    print(f"âŒ HTTP {response.status_code}: {url}")
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)  # å¤±æ•—æ™‚ã¯5ç§’å¾…æ©Ÿ
        return None
    
    def scrape_instagram_basic(self, username):
        """InstagramæŠ•ç¨¿ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
        try:
            url = f"https://www.instagram.com/{username}/"
            response = self.safe_request(url)
            
            if response:
                # æŠ•ç¨¿ã®ãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                post_pattern = r'/p/([A-Za-z0-9_-]+)/'
                matches = re.findall(post_pattern, response.text)
                
                # é‡è¤‡ã‚’é™¤å»ã—ã¦æœ€æ–°5ä»¶å–å¾—
                unique_posts = list(dict.fromkeys(matches))[:5]
                print(f"ğŸ“¸ Instagram @{username}: {len(unique_posts)}ä»¶ã®æŠ•ç¨¿ã‚’ç™ºè¦‹")
                
                for post_code in unique_posts:
                    post_data = {
                        'company': 'Rocket now' if 'rocketnow' in username.lower() else 'Wolt',
                        'platform': 'Instagram',
                        'username': username,
                        'post_url': f"https://www.instagram.com/p/{post_code}/",
                        'post_id': post_code,
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    self.data.append(post_data)
                    
        except Exception as e:
            print(f"âŒ Instagram {username} ã‚¨ãƒ©ãƒ¼: {e}")
    
    def scrape_twitter_nitter(self, username):
        """NitterçµŒç”±ã§TwitteræŠ•ç¨¿ã‚’å–å¾—"""
        try:
            # Nitterã®ãƒŸãƒ©ãƒ¼ã‚µã‚¤ãƒˆã‚’è©¦ã™
            nitter_instances = [
                f"https://nitter.net/{username}",
                f"https://nitter.it/{username}",
                f"https://nitter.privacydev.net/{username}"
            ]
            
            for nitter_url in nitter_instances:
                response = self.safe_request(nitter_url)
                
                if response:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    tweets = soup.find_all('div', class_='timeline-item')
                    
                    if tweets:
                        print(f"ğŸ¦ Twitter @{username}: {len(tweets[:5])}ä»¶ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ç™ºè¦‹")
                        
                        for tweet in tweets[:5]:  # æœ€æ–°5ä»¶
                            tweet_link = tweet.find('a', class_='tweet-link')
                            tweet_content = tweet.find('div', class_='tweet-content')
                            
                            if tweet_link and tweet_content:
                                # Nitterã®ãƒªãƒ³ã‚¯ã‚’æœ¬å®¶Twitterã®ãƒªãƒ³ã‚¯ã«å¤‰æ›
                                nitter_link = tweet_link.get('href', '')
                                twitter_link = f"https://twitter.com{nitter_link}"
                                
                                post_data = {
                                    'company': 'Rocket now' if 'rocketnow' in username.lower() else 'Wolt',
                                    'platform': 'Twitter',
                                    'username': username,
                                    'post_url': twitter_link,
                                    'content_preview': tweet_content.get_text(strip=True)[:100] + '...',
                                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                }
                                self.data.append(post_data)
                        break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    
        except Exception as e:
            print(f"âŒ Twitter {username} ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_existing_data(self):
        """æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡é˜²æ­¢ç”¨ï¼‰"""
        try:
            with open('data/posts.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return []
    
    def save_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        existing_data = self.load_existing_data()
        existing_urls = {item.get('post_url') for item in existing_data}
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
        new_data = [item for item in self.data 
                   if item.get('post_url') not in existing_urls]
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        all_data = existing_data + new_data
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        all_data.sort(key=lambda x: x.get('scraped_at', ''), reverse=True)
        
        # ä¿å­˜
        with open('data/posts.json', 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š çµæœ:")
        print(f"   æ–°è¦æŠ•ç¨¿: {len(new_data)}ä»¶")
        print(f"   ç·æŠ•ç¨¿æ•°: {len(all_data)}ä»¶")
        
        return len(new_data)

def main():
    print("ğŸš€ SNSæŠ•ç¨¿è‡ªå‹•å–å¾—ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("=" * 50)
    
    scraper = SNSScraper()
    
    # å–å¾—å¯¾è±¡ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
    accounts = [
        # Rocket now
        {'platform': 'instagram', 'username': 'rocketnow_official'},
        {'platform': 'twitter', 'username': 'Rocketnow_jp'},
        
        # Wolt
        {'platform': 'instagram', 'username': 'woltjapan'},
        {'platform': 'twitter', 'username': 'WoltJapan'}
    ]
    
    # å„ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰æŠ•ç¨¿ã‚’å–å¾—
    for account in accounts:
        print(f"\nğŸ“± {account['platform'].title()} @{account['username']} ã‚’å‡¦ç†ä¸­...")
        
        if account['platform'] == 'instagram':
            scraper.scrape_instagram_basic(account['username'])
        elif account['platform'] == 'twitter':
            scraper.scrape_twitter_nitter(account['username'])
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    new_posts_count = scraper.save_data()
    
    print("\n" + "=" * 50)
    print("âœ… å‡¦ç†å®Œäº†ï¼")
    
    if new_posts_count > 0:
        print(f"ğŸ‰ {new_posts_count}ä»¶ã®æ–°ã—ã„æŠ•ç¨¿ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
    else:
        print("ğŸ“­ æ–°ã—ã„æŠ•ç¨¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()
